{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import cumfreq, gaussian_kde\n",
    "\n",
    "\n",
    "# filter warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Function for Computing the Empirical Cumulative Distribution (ECDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ecdf(data):\n",
    "    data = np.asarray(data)\n",
    "    # Use cumfreq with number of bins equal to number of data points for a detailed ECDF.\n",
    "    res = cumfreq(data, numbins=len(data))\n",
    "    # Compute x values as the bin centers\n",
    "    x = res.lowerlimit + np.linspace(0, res.binsize*len(res.cumcount), len(res.cumcount))\n",
    "    y = res.cumcount / len(data)\n",
    "    return x, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEE Estimator Function\n",
    "\n",
    "- This is converted from the provided R code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def see_estimator(category, df):\n",
    "    # Filter by the given category (e.g., \"medA\")\n",
    "    df_cat = df[df['CATEGORY'] == category].copy()\n",
    "    \n",
    "    # Convert DATE column to datetime\n",
    "    df_cat['DATE'] = pd.to_datetime(df_cat['DATE'], format='%m/%d/%Y')\n",
    "    \n",
    "    # Sort by patient and date\n",
    "    df_cat.sort_values(by=['PATIENT_ID', 'DATE'], inplace=True)\n",
    "    \n",
    "    # Create a column for the previous fill date per patient\n",
    "    df_cat['prev_DATE'] = df_cat.groupby('PATIENT_ID')['DATE'].shift(1)\n",
    "    \n",
    "    # Keep only rows where a previous date exists\n",
    "    df_sample = df_cat.dropna(subset=['prev_DATE']).copy()\n",
    "    \n",
    "    # For each patient, randomly select one pair of consecutive prescriptions\n",
    "    df_sample = df_sample.groupby('PATIENT_ID').apply(lambda x: x.sample(1, random_state=1234)).reset_index(drop=True)\n",
    "    \n",
    "    # Keep only the needed columns\n",
    "    df_sample = df_sample[['PATIENT_ID', 'DATE', 'prev_DATE']].copy()\n",
    "    \n",
    "    # Compute the event interval (in days) between consecutive prescriptions\n",
    "    df_sample['event_interval'] = (df_sample['DATE'] - df_sample['prev_DATE']).dt.days.astype(float)\n",
    "    \n",
    "    # Compute the ECDF for event_interval using our ecdf function\n",
    "    x_ecdf, y_ecdf = ecdf(df_sample['event_interval'].values)\n",
    "    df_ecdf = pd.DataFrame({'x': x_ecdf, 'y': y_ecdf})\n",
    "    \n",
    "    # Plot the 80% ECDF (where y <= 0.8) and the full ECDF, with the medication label in the title\n",
    "    df_ecdf_80 = df_ecdf[df_ecdf['y'] <= 0.8]\n",
    "    ni = df_ecdf_80['x'].max()  # maximum x-value in the lower 80%\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(df_ecdf_80['x'], df_ecdf_80['y'], marker='.', linestyle='none')\n",
    "    plt.title(f'80% ECDF ({category})')\n",
    "    plt.xlabel('Event Interval (days)')\n",
    "    plt.ylabel('ECDF')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(df_ecdf['x'], df_ecdf['y'], marker='.', linestyle='none')\n",
    "    plt.title(f'100% ECDF ({category})')\n",
    "    plt.xlabel('Event Interval (days)')\n",
    "    plt.ylabel('ECDF')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Keep only those rows with event_interval less than or equal to ni\n",
    "    df_filtered = df_sample[df_sample['event_interval'] <= ni].copy()\n",
    "    \n",
    "    # Density estimation on the log-transformed event_interval\n",
    "    log_intervals = np.log(df_filtered['event_interval'])\n",
    "    kde = gaussian_kde(log_intervals)\n",
    "    x1 = np.linspace(log_intervals.min(), log_intervals.max(), 100)\n",
    "    y1 = kde(x1)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(x1, y1)\n",
    "    plt.title(f'Density of log(event_interval) ({category})')\n",
    "    plt.xlabel('log(Event Interval)')\n",
    "    plt.ylabel('Density')\n",
    "    plt.show()\n",
    "    \n",
    "    # --- Silhouette Analysis with Plot ---\n",
    "    silhouette_scores = []\n",
    "    k_values = range(2, 11)\n",
    "    # Prepare density data for silhouette analysis\n",
    "    a = np.column_stack((x1, y1))\n",
    "    scaler = StandardScaler()\n",
    "    a_scaled = scaler.fit_transform(a)\n",
    "    \n",
    "    for k in k_values:\n",
    "        kmeans_temp = KMeans(n_clusters=k, random_state=1234)\n",
    "        labels = kmeans_temp.fit_predict(a_scaled)\n",
    "        score = silhouette_score(a_scaled, labels)\n",
    "        silhouette_scores.append(score)\n",
    "    # Plot silhouette scores vs k\n",
    "    plt.figure()\n",
    "    plt.plot(list(k_values), silhouette_scores, marker='o')\n",
    "    plt.title(f'Silhouette Analysis for {category}')\n",
    "    plt.xlabel('Number of Clusters (k)')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.xticks(list(k_values))\n",
    "    plt.show()\n",
    "    \n",
    "    # Determine the optimal number of clusters based on maximum silhouette score\n",
    "    best_k = k_values[np.argmax(silhouette_scores)]\n",
    "    print(f\"Optimal number of clusters (silhouette analysis) for {category}: {best_k}\")\n",
    "    \n",
    "    # K-means clustering on the ECDF x values (event_interval values from df_ecdf)\n",
    "    kmeans_final = KMeans(n_clusters=best_k, random_state=1234)\n",
    "    df_ecdf['cluster'] = kmeans_final.fit_predict(df_ecdf[['x']])\n",
    "    \n",
    "    # For each cluster, compute the min, max, and median of log(x), then exponentiate to revert to days\n",
    "    cluster_stats = df_ecdf.groupby('cluster')['x'].agg(\n",
    "        min_log=lambda x: np.log(x).min(),\n",
    "        max_log=lambda x: np.log(x).max(),\n",
    "        median_log=lambda x: np.log(x).median()\n",
    "    ).reset_index()\n",
    "    cluster_stats['Minimum'] = np.exp(cluster_stats['min_log'])\n",
    "    cluster_stats['Maximum'] = np.exp(cluster_stats['max_log'])\n",
    "    cluster_stats['Median'] = np.exp(cluster_stats['median_log'])\n",
    "    # Keep only clusters with a positive median\n",
    "    cluster_stats = cluster_stats[cluster_stats['Median'] > 0]\n",
    "    \n",
    "    print(\"Cluster Statistics:\")\n",
    "    print(cluster_stats[['cluster', 'Minimum', 'Maximum', 'Median']])\n",
    "    \n",
    "    # Function to assign a cluster based on an event_interval value\n",
    "    def assign_cluster(interval):\n",
    "        for _, row in cluster_stats.iterrows():\n",
    "            if interval >= row['Minimum'] and interval <= row['Maximum']:\n",
    "                return row['cluster']\n",
    "        return np.nan\n",
    "    \n",
    "    df_sample['Final_cluster'] = df_sample['event_interval'].apply(assign_cluster)\n",
    "    df_sample = df_sample.dropna(subset=['Final_cluster']).copy()\n",
    "    \n",
    "    # Merge the cluster median (estimated duration) into df_sample\n",
    "    df_sample = df_sample.merge(cluster_stats[['cluster', 'Median']], left_on='Final_cluster', right_on='cluster', how='left', suffixes=('', '_cluster'))\n",
    "    \n",
    "    # Determine the most frequent cluster among the sampled pairs\n",
    "    if not df_sample.empty:\n",
    "        most_freq_cluster = df_sample['Final_cluster'].value_counts().idxmax()\n",
    "        default_median = cluster_stats.loc[cluster_stats['cluster'] == most_freq_cluster, 'Median'].values[0]\n",
    "    else:\n",
    "        default_median = np.nan\n",
    "    \n",
    "    # Use the computed median as the estimated duration\n",
    "    df_sample['Median_est'] = df_sample['Median'].fillna(default_median)\n",
    "    \n",
    "    # Merge the estimated duration back into the original category dataset\n",
    "    df_result = df_cat.merge(df_sample[['PATIENT_ID', 'Median_est', 'Final_cluster']], on='PATIENT_ID', how='left')\n",
    "    df_result['Median_est'] = df_result['Median_est'].fillna(default_median)\n",
    "    df_result['Final_cluster'] = df_result['Final_cluster'].fillna(0)\n",
    "    \n",
    "    return df_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEE Assumption Visualization Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def see_assumption(df):\n",
    "    # Convert DATE to datetime\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'], format='%m/%d/%Y')\n",
    "    \n",
    "    # Sort data by patient and date\n",
    "    df_sorted = df.sort_values(by=['PATIENT_ID', 'DATE']).copy()\n",
    "    \n",
    "    # Create a sequential prescription number for each patient\n",
    "    df_sorted['p_number'] = df_sorted.groupby('PATIENT_ID').cumcount() + 1\n",
    "    \n",
    "    # Compute the previous date and the duration between prescriptions\n",
    "    df_sorted['prev_DATE'] = df_sorted.groupby('PATIENT_ID')['DATE'].shift(1)\n",
    "    df_sorted['Duration'] = (df_sorted['DATE'] - df_sorted['prev_DATE']).dt.days\n",
    "    \n",
    "    # Retain only rows where p_number is 2 or more\n",
    "    df_assump = df_sorted[df_sorted['p_number'] >= 2].copy()\n",
    "    \n",
    "    # Compute median duration per patient\n",
    "    medians_of_medians = df_assump.groupby('PATIENT_ID')['Duration'].median().reset_index(name='median_duration')\n",
    "    \n",
    "    # Plot a boxplot of Duration by prescription order with a horizontal line at the overall median\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x='p_number', y='Duration', data=df_assump)\n",
    "    overall_median = medians_of_medians['median_duration'].median()\n",
    "    plt.axhline(y=overall_median, linestyle='--', color='red', label=f'Overall median = {overall_median:.1f}')\n",
    "    plt.title('Duration by Prescription Order')\n",
    "    plt.xlabel('Prescription Order (p_number)')\n",
    "    plt.ylabel('Duration (days)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return plt.gcf()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading of Data and Running of SEE Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data from the CSV file located at \"data/med_events.csv\"\n",
    "df = pd.read_csv(\"data/med_events.csv\")\n",
    "\n",
    "# Inspect the first few rows of the data\n",
    "df.tail()\n",
    "\n",
    "# Compute SEE estimates for specific medication categories, e.g., \"medA\" and \"medB\"\n",
    "medA_estimates = see_estimator(\"medA\", df)\n",
    "medB_estimates = see_estimator(\"medB\", df)\n",
    "\n",
    "# Visualize the assumption check for the entire dataset\n",
    "fig = see_assumption(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
